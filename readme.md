# Docker Pruna - Download e Compilazione Modelli Diffusers

Questo progetto fornisce un ambiente Docker parametrizzabile per scaricare e compilare modelli diffusion con l'ottimizzazione Pruna per inferenze pi√π veloci.

## üöÄ Caratteristiche Principali

- **Parametrizzabile**: Configura facilmente modello, directory di download e compilazione
- **Flessibile**: Supporta diversi modelli da Hugging Face
- **Ottimizzato**: Usa Pruna per accelerare le inferenze
- **Docker-ready**: Container pronto per produzione

## üìã Parametri Configurabili

Il nuovo script `main.py` accetta questi parametri:

### Variabili d'Ambiente
- `MODEL_DIFF`: ID del modello su Hugging Face (default: `CompVis/stable-diffusion-v1-4`)
- `DOWNLOAD_DIR`: Directory per scaricare i modelli (default: `./models`)
- `PRUNA_COMPILED_DIR`: Directory per salvare i modelli compilati (default: `./compiled_models`)

### Argomenti CLI
```bash
python3 main.py --help

optional arguments:
  --model-id MODEL_ID    Hugging Face model ID to download
  --download-dir DIR     Directory to download models
  --compiled-dir DIR     Directory to save compiled Pruna models
  --skip-download        Skip download step (use existing model)
  --skip-compile         Skip compilation step (only download)
  --torch-dtype TYPE     Torch dtype for model loading (float16/float32)
```

## üõ†Ô∏è Utilizzo

### Docker Build con Parametri

```bash
# Build con modello di default
docker build -t docker-pruna .

# Build con modello personalizzato
docker build --build-arg MODEL_DIFF="runwayml/stable-diffusion-v1-5" -t docker-pruna .

# Build con variabili d'ambiente
docker build \
  --build-arg MODEL_DIFF="stabilityai/stable-diffusion-2-1" \
  --build-arg DOWNLOAD_DIR="/app/custom_models" \
  --build-arg PRUNA_COMPILED_DIR="/app/custom_compiled" \
  -t docker-pruna .
```

### Uso Locale

```bash
# Esempio 1: Verifica automatica - se il modello esiste, non fa nulla
python3 download_model_and_compile.py --model-id runwayml/stable-diffusion-v1-5

# Esempio 2: Scarica e compila un nuovo modello con modalit√† veloce  
python3 download_model_and_compile.py --model-id CompVis/stable-diffusion-v1-4 --compilation-mode fast

# Esempio 3: Usa cartelle personalizzate
python3 download_model_and_compile.py --model-id <MODEL_ID> --download-dir /custom/path --compiled-dir /custom/compiled

```

### Test

```bash
# Esegui tutti i test
./test_main.sh

# Test manuale del modello compilato
python3 test_pruna_infer.py
```

## üìÅ Struttura Directory

```
/app/
‚îú‚îÄ‚îÄ main.py                    # Script principale parametrizzabile
‚îú‚îÄ‚îÄ models/                    # Modelli scaricati originali
‚îÇ   ‚îî‚îÄ‚îÄ CompVis--stable-diffusion-v1-4/
‚îú‚îÄ‚îÄ compiled_models/           # Modelli ottimizzati con Pruna
‚îÇ   ‚îî‚îÄ‚îÄ CompVis--stable-diffusion-v1-4/
‚îú‚îÄ‚îÄ test_pruna_infer.py       # Test di inferenza
‚îî‚îÄ‚îÄ test_main.sh              # Script di test
```

## üîß Configurazione Avanzata

### Dockerfile Personalizzato

Modifica il Dockerfile per cambiare i default:

```dockerfile
# Cambia modello di default
ENV MODEL_DIFF=runwayml/stable-diffusion-v1-5
ENV DOWNLOAD_DIR=/app/models  
ENV PRUNA_COMPILED_DIR=/app/compiled_models
```

### ComfyUI Integration

Il modello compilato sar√† disponibile in `/app/compiled_models/` e pu√≤ essere utilizzato direttamente dai nodi Pruna in ComfyUI configurando il path appropriato.

## üéØ Cosa Succede all'Avvio Container

1. **Download**: Il modello specificato viene scaricato da Hugging Face
2. **Compilazione**: Pruna ottimizza il modello per inferenze pi√π veloci  
3. **Ready**: Il modello ottimizzato √® disponibile per l'uso immediato

L'inferenza parte istantaneamente senza compilazioni ripetute!

## üí° Suggerimenti

- Usa `--skip-download` se hai gi√† il modello scaricato
- Usa `--skip-compile` per solo scaricare il modello
- Sincronizza le versioni di PyTorch/Pruna tra build e runtime
- Considera multi-stage builds per container di produzione pi√π piccoli

## üö® Requisiti

- CUDA 12.1+ per GPU support
- Python 3.8+
- Spazio disco sufficiente per modelli (3-7GB per modello)
- Token Hugging Face per modelli privati (opzionale)a for Faster inferences

# Docker Pruna - Download e Compilazione Modelli Diffusers

Questo progetto fornisce un ambiente Docker parametrizzabile per scaricare e compilare modelli diffusion con l'ottimizzazione Pruna per inferenze pi√π veloci.

## üöÄ Caratteristiche Principali

- **Parametrizzabile**: Configura facilmente modello, directory di download e compilazione
- **Tre Modalit√† di Compilazione**: Fast, Moderate, Normal per bilanciare velocit√† vs qualit√†
- **Flessibile**: Supporta diversi modelli da Hugging Face
- **Ottimizzato**: Usa Pruna per accelerare le inferenze
- **Docker-ready**: Container pronto per produzione

## üìã Parametri Configurabili

Il script `main.py` accetta questi parametri:

### Variabili d'Ambiente
- `MODEL_DIFF`: ID del modello su Hugging Face (default: `CompVis/stable-diffusion-v1-4`)
- `DOWNLOAD_DIR`: Directory per scaricare i modelli (default: `./models`)
- `PRUNA_COMPILED_DIR`: Directory per salvare i modelli compilati (default: `./compiled_models`)

### Argomenti CLI
```bash
python3 main.py --help

optional arguments:
  --model-id MODEL_ID              Hugging Face model ID to download
  --download-dir DIR               Directory to download models
  --compiled-dir DIR               Directory to save compiled Pruna models
  --skip-download                  Skip download step (use existing model)
  --skip-compile                   Skip compilation step (only download)
  --torch-dtype TYPE               Torch dtype for model loading (float16/float32)
  --compilation-mode MODE          Pruna compilation mode (fast/moderate/normal)
```

## üéõÔ∏è Modalit√† di Compilazione

### üöÄ Fast (Veloce)
**Uso**: Prototipazione rapida, test, sviluppo
- **Caching**: DeepCache con interval 3 (pi√π veloce)
- **Compiler**: stable_fast (ottimizzazioni rapide)
- **Quantization**: half (FP16, leggera)
- **Tempo**: ~5-10 minuti
- **Qualit√†**: Buona, perdita minima di qualit√†

### ‚öñÔ∏è Moderate (Moderata)
**Uso**: Produzione bilanciata, deploy standard
- **Caching**: DeepCache con interval 2 (bilanciato)
- **Compiler**: torch_compile con mode default
- **Quantization**: HQQ 8-bit con group size 64
- **Tempo**: ~15-25 minuti
- **Qualit√†**: Ottima, rapporto qualit√†/velocit√† ideale

### üéØ Normal (Completa)
**Uso**: Massima qualit√†, produzione critica
- **Caching**: FORA avanzato con factorization
- **Factorizer**: QKV diffusers per ottimizzazioni complete
- **Compiler**: torch_compile con max-autotune
- **Quantization**: HQQ 4-bit con backend torchao_int4
- **Tempo**: ~30-60 minuti
- **Qualit√†**: Massima, ottimizzazioni complete

## üåê API Server

Il progetto include un server Flask per l'inferenza via API REST. Il server carica automaticamente i modelli compilati con Pruna per prestazioni ottimizzate.

### Avvio del Server

```bash
# Avvia il server (localhost:5000)
python3 server.py
```

### Endpoint /generate - Esempi CURL

#### Generazione Base
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "stable-diffusion-v1-5",
    "prompt": "A beautiful sunset over the ocean"
  }'
```

#### Generazione con Parametri Avanzati
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "stable-diffusion-v1-5",
    "prompt": "A futuristic city with flying cars",
    "negative_prompt": "blurry, low quality, dark",
    "num_inference_steps": 30,
    "guidance_scale": 7.5,
    "width": 768,
    "height": 768,
    "num_images": 2
  }'
```

#### Generazione con Debug (Salvataggio Locale)
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "stable-diffusion-v1-5",
    "prompt": "A magical forest with glowing trees",
    "debug": true,
    "num_images": 3
  }'
```

#### Generazione Rapida per Test
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "stable-diffusion-v1-5",
    "prompt": "A red car",
    "num_inference_steps": 10,
    "width": 512,
    "height": 512
  }'
```

#### Generazione ad Alta Qualit√†
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "stable-diffusion-v1-5",
    "prompt": "A detailed portrait of a wise old wizard",
    "negative_prompt": "blurry, low quality, deformed",
    "num_inference_steps": 50,
    "guidance_scale": 12.0,
    "width": 1024,
    "height": 1024
  }'
```

#### Parametri Supportati

| Parametro | Tipo | Default | Descrizione |
|-----------|------|---------|-------------|
| `model_id` | string | **richiesto** | ID del modello (nome cartella in compiled_models/) |
| `prompt` | string | **richiesto** | Prompt di generazione |
| `negative_prompt` | string | `""` | Prompt negativo (cosa evitare) |
| `num_inference_steps` | int | `20` | Numero di step di denoising (10-100) |
| `guidance_scale` | float | `7.5` | Aderenza al prompt (1.0-20.0) |
| `width` | int | `512` | Larghezza immagine (multipli di 8) |
| `height` | int | `512` | Altezza immagine (multipli di 8) |
| `num_images` | int | `1` | Numero di immagini da generare |
| `debug` | bool | `false` | Salva immagini in locale (./generated_images/) |

#### Risposta API

```json
{
  "status": "success",
  "images": ["base64_encoded_image_1", "base64_encoded_image_2"],
  "prompt": "A magical forest with glowing trees",
  "model_used": "stable-diffusion-v1-5",
  "inference_time": 2.34,
  "saved_files": ["./generated_images/20250813_143022_img_00.png"]
}
```

**Note:**
- Le immagini sono codificate in base64 nel campo `images`
- Con `debug: true`, le immagini vengono salvate in `./generated_images/`
- Il server usa automaticamente modelli Pruna se disponibili per prestazioni ottimizzate
- Tempi di risposta tipici: 1-5 secondi (dipende da parametri e hardware)

## üõ†Ô∏è Utilizzo

### Docker Build con Parametri

```bash
# Build con modello di default (modalit√† normal)
docker build -t docker-pruna .

# Build con modalit√† veloce
docker build --build-arg COMPILATION_MODE=fast -t docker-pruna .

# Build con modello personalizzato e modalit√† moderata
docker build 
  --build-arg MODEL_DIFF="runwayml/stable-diffusion-v1-5" 
  --build-arg COMPILATION_MODE=moderate 
  -t docker-pruna .
```

### Uso Locale

```bash
# Modalit√† normale (default)
python3 main.py

# Modalit√† veloce
python3 main.py --compilation-mode fast

# Modalit√† moderata con modello personalizzato
python3 main.py --model-id "runwayml/stable-diffusion-v1-5" --compilation-mode moderate

# Solo download
python3 main.py --skip-compile

# Solo compilazione modalit√† veloce (modello esistente)
python3 main.py --skip-download --compilation-mode fast

# Con variabili d'ambiente
export MODEL_DIFF="stabilityai/stable-diffusion-2-1"
export DOWNLOAD_DIR="./my_models"
export PRUNA_COMPILED_DIR="./my_compiled"
python3 main.py --compilation-mode moderate
```

### Test

```bash
# Esegui tutti i test
./test_main.sh

# Test manuale del modello compilato
python3 test_pruna_infer.py
```

## üìÅ Struttura Directory

```
/app/
‚îú‚îÄ‚îÄ main.py                    # Script principale parametrizzabile
‚îú‚îÄ‚îÄ models/                    # Modelli scaricati originali
‚îÇ   ‚îî‚îÄ‚îÄ CompVis--stable-diffusion-v1-4/
‚îú‚îÄ‚îÄ compiled_models/           # Modelli ottimizzati con Pruna
‚îÇ   ‚îî‚îÄ‚îÄ CompVis--stable-diffusion-v1-4/
‚îú‚îÄ‚îÄ test_pruna_infer.py       # Test di inferenza
‚îî‚îÄ‚îÄ test_main.sh              # Script di test
```

## üéØ Cosa Succede all'Avvio Container

1. **Download**: Il modello specificato viene scaricato da Hugging Face
2. **Compilazione**: Pruna ottimizza il modello secondo la modalit√† scelta
3. **Ready**: Il modello ottimizzato √® disponibile per l'uso immediato

L'inferenza parte istantaneamente senza compilazioni ripetute!

## üîß Configurazione Avanzata

### ComfyUI Integration

Il modello compilato sar√† disponibile in `/compiled_models/` e pu√≤ essere utilizzato direttamente dai nodi Pruna in ComfyUI configurando il path appropriato.

### Dockerfile Personalizzato

Modifica il Dockerfile per cambiare i default:

```dockerfile
# Cambia modello e modalit√† di default
ENV MODEL_DIFF=runwayml/stable-diffusion-v1-5
ENV DOWNLOAD_DIR=/app/models  
ENV PRUNA_COMPILED_DIR=/app/compiled_models
```

## üí° Suggerimenti

### Scegliere la Modalit√† Giusta

- **Fast**: Per sviluppo, test rapidi, prototipazione
- **Moderate**: Per la maggior parte dei casi di produzione
- **Normal**: Per applicazioni critiche dove la qualit√† √® prioritaria

### Performance Tips

- Usa `--skip-download` se hai gi√† il modello scaricato
- Usa `--skip-compile` per solo scaricare il modello  
- Sincronizza le versioni di PyTorch/Pruna tra build e runtime
- Considera multi-stage builds per container di produzione pi√π piccoli
- La modalit√† Fast pu√≤ accelerare la compilazione del 70-80%
- La modalit√† Normal offre fino al 30% di miglioramento nelle prestazioni di inferenza

## üö® Requisiti

- CUDA 12.1+ per GPU support (richiesto per stable_fast e HQQ)
- Python 3.8+
- Spazio disco sufficiente per modelli (3-7GB per modello)
- Token Hugging Face per modelli privati (opzionale)
- 8GB+ RAM per modalit√† Normal con modelli grandi

## üìä Confronto Modalit√†

| Modalit√† | Tempo Compilazione | Qualit√† Output | Velocit√† Inferenza | Uso Consigliato |
|----------|-------------------|----------------|-------------------|------------------|
| Fast     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê            | ‚≠ê‚≠ê‚≠ê‚≠ê          | ‚≠ê‚≠ê‚≠ê‚≠ê              | Sviluppo, Test   |
| Moderate | ‚≠ê‚≠ê‚≠ê              | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê            | Produzione       |
| Normal   | ‚≠ê                 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê            | Critico          |

## Suggerimenti

Se vuoi integrare tutto nella pipeline ComfyUI, aggiungi le estensioni/plugin Pruna con COPY . e assicurati che nella tua UI si possa configurare il path /compiled_models/.

Sincronizza le versioni di torch/Pruna tra build e run.

Puoi dividere build & run in multistage Docker se necessario, ma in casi semplici lo script sopra funziona direttamente.

Cos√¨ ottieni un container "ready-to-go": il modello viene scaricato, compilato da Pruna, e ComfyUI/Pruna Node reimpiega direttamente la compilazione per le inferenze.